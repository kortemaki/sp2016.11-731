Keith Maki
11-731 Machine Translation
Homework 3
4/1/16

In this homework, I explored simulated annealing based approaches to the given search problem, settling on an approach with the following properties:

At each timestep, an e' is generated by swapping two adjacent (random length) blocks of words in the source sentence.

The proposed e' is accepted with probability:
              {        1       |  e' > e
P(e, e', T) = {                |
              { exp(-(e-e')/T) |   else

Otherwise, e' is rejected and the next timestep will reconsider e.

At each timestep, the temperature (parameter T) cools according to the cooling rate (parameter d), decreasing the free energy in the system and approaching a stable sample from the distribution.

The system will also explore a number of random restarts (parameter k) which reset the simulation to the most ideal condition seen previously.

Simulated annealing is a form of Metropolis Hastings sampling, in which a difficult-to-sample distribution is approximated exactly in the limit for slowly cooling, high-temperature simulations.

I submit results from the following configuration, which may be outperformed by higher temperature, slower cooling simulations:

--stack-size=10 --restarts=300 --temperature=3 --time-const=200

These results attain a final model score of -5035.91

-------------- File Specifics --------------

There are three Python programs here (`-h` for usage):

 - `./decode` a simple non-reordering (monotone) phrase-based decoder
 - './anneal' a simulated annealing-based phrase-based decoder
 - `./grade` computes the model score of your output

The commands are designed to work in a pipeline. For instance, this is a valid invocation:

    ./anneal | ./grade

The `data/` directory contains the input set to be decoded and the models

 - `data/input` is the input text

 - `data/lm` is the ARPA-format 3-gram language model

 - `data/tm` is the phrase translation model

